{
    "contents" : "\n#Ricardo, este é o script que usei para validação do SST, pode servir como modelo para EFA e CFA.\n#Coloquei no final comandos para IRT também\n###\n\nsetwd(\"C:/Users/user/Google Drive/Projeto_Duke/SST Validation\")\ndata<-read.csv(\"sstdata.csv\",header=T)\ndata1<-data.frame(data$q11,data$q12,data$q13,data$q14,data$q15,data$q16,data$q17,data$q18,data$q19,data$q20,data$q21,data$q22)\ndata2<-read.csv(\"sstdata1.csv\",header=T)\n#data<-na.omit(data1)\n\n#Instal packages needes for the analysis\nlapply(c(\"ggplot2\", \"psych\", \"RCurl\", \"irr\", \"nortest\", \"moments\"), library, character.only=T)\n\n#uploading data ------------------------------------------------------------------------\n#Functions to pull the dara from the internet file \noptions(RCurlOptions = list(capath = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\"), ssl.verifypeer = FALSE))\n#see http://goo.gl/mQwxO on how to get this link\n#link below won't work until data is entered in the right format\ndata <- getURL(\"https://docs.google.com/spreadsheet/pub?hl=en&hl=en&key=0AoTReYGK49h_dEFHWXVfODR0NlZWdXFoVTZjT09oc0E&single=true&gid=3&output=csv\")\ndataicc<-read.csv(textConnection(data))\nnames(preference.tto)\n\noptions(RCurlOptions = list(capath = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\"), ssl.verifypeer = FALSE))\n#see http://goo.gl/mQwxO on how to get this link\n#link below won't work until data is entered in the right format\ndata4 <- getURL(\"https://docs.google.com/spreadsheet/pub?hl=en&hl=en&key=0AoTReYGK49h_dEFHWXVfODR0NlZWdXFoVTZjT09oc0E&single=true&gid=4&output=csv\")\ndatacor<-read.csv(textConnection(data4))\nnames(preference.tto)\n\n\n##########################################################################################################################\n#Exploratory Data Anlysis\ndim(data)\nsummary(data)\n\n\n##########################################################################################################################\n#EFA\n#Determine the number of factors\nlibrary(nFactors)\n\npar(mfrow=c(2,2))\nev <- eigen(cor(data)) # get eigenvalues\nev\nap <- parallel(subject=nrow(data),var=ncol(data),rep=100,cent=.05)\nnS <- nScree(ev$values)\nplotnScree(nS)\n\n#KMO\nkmo = function( data1 ){\n  \n  library(MASS)\n  X <- cor(as.matrix(data1))\n  iX <- ginv(X)\n  S2 <- diag(diag((iX^-1)))\n  AIS <- S2%*%iX%*%S2                      # anti-image covariance matrix\n  IS <- X+AIS-2*S2                         # image covariance matrix\n  Dai <- sqrt(diag(diag(AIS)))\n  IR <- ginv(Dai)%*%IS%*%ginv(Dai)         # image correlation matrix\n  AIR <- ginv(Dai)%*%AIS%*%ginv(Dai)       # anti-image correlation matrix\n  a <- apply((AIR - diag(diag(AIR)))^2, 2, sum)\n  AA <- sum(a)\n  b <- apply((X - diag(nrow(X)))^2, 2, sum)\n  BB <- sum(b)\n  MSA <- b/(b+a)                        # indiv. measures of sampling adequacy\n  \n  AIR <- AIR-diag(nrow(AIR))+diag(MSA)  # Examine the anti-image of the\n  # correlation matrix. That is the\n  # negative of the partial correlations,\n  # partialling out all other variables.\n  \n  kmo <- BB/(AA+BB)                     # overall KMO statistic\n  \n  # Reporting the conclusion\n  if (kmo >= 0.00 && kmo < 0.50){\n    test <- 'The KMO test yields a degree of common variance\nunacceptable for FA.'\n  } else if (kmo >= 0.50 && kmo < 0.60){\n    test <- 'The KMO test yields a degree of common variance miserable.'\n  } else if (kmo >= 0.60 && kmo < 0.70){\n    test <- 'The KMO test yields a degree of common variance mediocre.'\n  } else if (kmo >= 0.70 && kmo < 0.80){\n    test <- 'The KMO test yields a degree of common variance middling.'\n  } else if (kmo >= 0.80 && kmo < 0.90){\n    test <- 'The KMO test yields a degree of common variance meritorious.'\n  } else {\n    test <- 'The KMO test yields a degree of common variance marvelous.'\n  }\n  \n  ans <- list(  overall = kmo,\n                report = test,\n                individual = MSA,\n                AIS = AIS,\n                AIR = AIR )\n  return(ans)\n  \n}    # end of kmo()\nkmo(data)\n\n#FACTOR EXTRACTION\nlibrary(psych)\nlibrary(GPArotation)\n\nfa(data1,3,fm=\"pa\",rotate=\"promax\")\nfa(data1,1,fm=\"pa\",rotate=\"promax\")\nfa(data1,2,fm=\"pa\",rotate=\"promax\")\n\n#######################################################################################################\n#CFA Models\n# Sem model\nlibrary (sem)\n\n#1 Factor Model ############################################\n\nmod.1 <- specifyModel()# Type these values that specify the model's relations (just use de Ctrl+R over each relation).\n\n#Latent Variables\nSST->data.q11,var3,NA\nSST->data.q12,var4,NA\nSST->data.q13,var5,NA\nSST->data.q14,var6,NA\nSST->data.q15,var7,NA\nSST->data.q16,var8,NA\nSST->data.q17,var9,NA\nSST->data.q18,var10,NA\nSST->data.q19,var11,NA\nSST->data.q20,var12,NA\nSST->data.q21,var13,NA\nSST->data.q22,var14,NA\n\n#Erros and COv\nSST<->SST,NA,1\ndata.q11<->data.q11,err3,NA\ndata.q12<->data.q12,err4,NA\ndata.q13<->data.q13,err5,NA\ndata.q14<->data.q14,err6,NA\ndata.q15<->data.q15,err7,NA\ndata.q16<->data.q16,err8,NA\ndata.q17<->data.q17,err9,NA\ndata.q18<->data.q18,err10,NA\ndata.q19<->data.q19,err11,NA\ndata.q20<->data.q20,err12,NA\ndata.q21<->data.q21,err13,NA\ndata.q22<->data.q22,err14,NA\n\n# Insert de covariance matrix\ncov <- cov(data2, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))\ncov\ncor(data1)\nsem.1 <- sem(mod.1, cov, N=100)\nfscore(sem.wh.2)\n\nsummary(sem.1)\n\nmodIndices(sem.1)\n\n#2 Factor Model ##################################################\n\nmod.wh.2 <- specifyModel()# Type these values that specify the model's relations (just use de Ctrl+R over each relation).\n\n#Latent Variables\nSST3->data.q11,var3,NA\nSST3->data.q12,var4,NA\nSST3->data.q13,var5,NA\nSST3->data.q14,var6,NA\nSST1->data.q15,var7,NA\nSST1->data.q16,var8,NA\nSST1->data.q17,var9,NA\nSST2->data.q18,var10,NA\nSST1->data.q19,var11,NA\nSST2->data.q20,var12,NA\nSST1->data.q21,var13,NA\nSST2->data.q22,var14,NA\n\n#Erros and COv\nSST1<->SST1,NA,1\nSST2<->SST2,NA,1\nSST3<->SST3,NA,1\ndata.q11<->data.q11,err3,NA\ndata.q12<->data.q12,err4,NA\ndata.q13<->data.q13,err5,NA\ndata.q14<->data.q14,err6,NA\ndata.q15<->data.q15,err7,NA\ndata.q16<->data.q16,err8,NA\ndata.q17<->data.q17,err9,NA\ndata.q18<->data.q18,err10,NA\ndata.q19<->data.q19,err11,NA\ndata.q20<->data.q20,err12,NA\ndata.q21<->data.q21,err13,NA\ndata.q22<->data.q22,err14,NA\ndata.q14<->data.q13,cov1,NA\ndata.q20->data.q14,cov3,NA\nSST1<->SST2,lat1,NA\nSST1<->SST3,lat2,NA\nSST2<->SST3,lat3,NA\ndata.q22<->data.q21,cov4,NA\nSST<->SST,lat4,NA\n\n# Insert de covariance matrix\ncov <- cov(data1, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))\ncov\ncor(data1)\nsem.wh.2 <- sem(mod.wh.2, cov, N=100)\nfscore(sem.wh.2)\n\nsummary(sem.wh.2)\n\nmod.indices(sem.wh.2)\n\n# 3 Factor Model ################################################\n\nmod.3 <- specifyModel()# Type these values that specify the model's relations (just use de Ctrl+R over each relation).\n\n#Latent Variables\nSST3->data.q11,var3,NA\nSST3->data.q12,var4,NA\nSST3->data.q13,var5,NA\nSST3->data.q14,var6,NA\nSST1->data.q15,var7,NA\nSST1->data.q16,var8,NA\nSST1->data.q17,var9,NA\nSST2->data.q18,var10,NA\nSST1->data.q19,var11,NA\nSST2->data.q20,var12,NA\nSST1->data.q21,var13,NA\nSST2->data.q22,var14,NA\n\n#Erros and COv\nSST1<->SST1,NA,1\nSST2<->SST2,NA,1\nSST3<->SST3,NA,1\ndata.q11<->data.q11,err3,NA\ndata.q12<->data.q12,err4,NA\ndata.q13<->data.q13,err5,NA\ndata.q14<->data.q14,err6,NA\ndata.q15<->data.q15,err7,NA\ndata.q16<->data.q16,err8,NA\ndata.q17<->data.q17,err9,NA\ndata.q18<->data.q18,err10,NA\ndata.q19<->data.q19,err11,NA\ndata.q20<->data.q20,err12,NA\ndata.q21<->data.q21,err13,NA\ndata.q22<->data.q22,err14,NA\ndata.q14<->data.q13,cov1,NA\ndata.q20->data.q14,cov3,NA\nSST1<->SST2,lat1,NA\nSST1<->SST3,lat2,NA\nSST2<->SST3,lat3,NA\ndata.q22<->data.q21,cov4,NA\n\n# Insert de covariance matrix\ncov <- cov(data1, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))\ncov\ncor(data1)\nsem.3 <- sem(mod.3, cov, N=100)\neffects(sem.3)\nstandardizedCoefficients(sem.3)\nstandardizedResiduals(sem.3)\nsummary(sem.3)\n\nmod.indices(sem.wh.2)\n\n#2o Order Model#######################################################\n# 3 Factor Model\n\nmod.4 <- specifyModel()# Type these values that specify the model's relations (just use de Ctrl+R over each relation).\n\n#Latent Variables\nSST3->data.q11,var3,NA\nSST3->data.q12,var4,NA\nSST3->data.q13,var5,NA\nSST3->data.q14,var6,NA\nSST1->data.q15,var7,NA\nSST1->data.q16,var8,NA\nSST1->data.q17,var9,NA\nSST2->data.q18,var10,NA\nSST1->data.q19,var11,NA\nSST2->data.q20,var12,NA\nSST1->data.q21,var13,NA\nSST2->data.q22,var14,NA\nSST->SST1,NA,1\nSST->SST2,lat1,NA\nSST->SST3,lat2,NA\n\n#Erros and COv\nSST1<->SST1,NA,1\nSST2<->SST2,NA,1\nSST3<->SST3,NA,1\nSST<->SST,erro1,NA\ndata.q11<->data.q11,err3,NA\ndata.q12<->data.q12,err4,NA\ndata.q13<->data.q13,err5,NA\ndata.q14<->data.q14,err6,NA\ndata.q15<->data.q15,err7,NA\ndata.q16<->data.q16,err8,NA\ndata.q17<->data.q17,err9,NA\ndata.q18<->data.q18,err10,NA\ndata.q19<->data.q19,err11,NA\ndata.q20<->data.q20,err12,NA\ndata.q21<->data.q21,err13,NA\ndata.q22<->data.q22,err14,NA\ndata.q14<->data.q13,cov1,NA\ndata.q20->data.q14,cov3,NA\n#SST1<->SST2,lat1,NA\n#SST1<->SST3,lat2,NA\n#SST2<->SST3,lat3,NA\n#data.q22<->data.q21,cov4,NA\n\n# Insert de covariance matrix\ncov <- cov(data1, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))\ncov\ncor(data1)\nsem.4 <- sem(mod.4, cov, N=100)\neffects(sem.3)\nstandardizedCoefficients(sem.4)\nstandardizedResiduals(sem.3)\nsummary(sem.4)\n\nmodIndices(sem.4)\n\n#################################################################################################\n\n#Alpha de Cronbach by psych package\nalpha(data1, keys=NULL,title=NULL,na.rm = TRUE)\nalpha(data2, keys=NULL,title=NULL,na.rm = TRUE)\n\n\n#Alpha de Cronbach by ltm package\ninstall.packages(\"ltm\")\nlibrary(ltm)\ncronbach.alpha(data2$PA, standardized = TRUE, CI = TRUE, \n               probs = c(0.025, 0.975), B = 1000, na.rm = FALSE)\n\n#################################################################################################\n\n# ICC by psy package\n\ninstall.packages(\"psy\")\nlibrary(psy)\n\nx<-data(expsy)\nicc(expsy[,c(12,14,16)])\n\n# ICC by irr package\nattach(dataicc)\nsummary(dataicc)\nPAICC<-data.frame(PA,PA2)\nADLICC<-data.frame(ADL,ADL2)\nCRICC<-data.frame(CR,CR2)\nVGICC<-data.frame(PA,PA2)\n\ninstall.packages(\"irr\")\nlibrary(irr)\n\ncc(ratings, model = c(\"oneway\", \"twoway\"), \n   type = c(\"consistency\", \"agreement\"), \n   unit = c(\"single\", \"average\"), r0 = 0, conf.level = 0.95)\n\ndata(dataicc)\nicc(PAICC, model=\"twoway\", type=\"agreement\")\nicc(ADLICC, model=\"twoway\", type=\"agreement\")\nicc(CRICC, model=\"twoway\", type=\"agreement\")\nicc(VGICC, model=\"twoway\", type=\"agreement\")\n\n###########################################################\n#Correlation with SF-36\n\nsummary(datacor)\ndetach(dataicc)\ndetach(datacor)\nattach(datacor)       \nsst<-data.frame(PA,ADL,CR,VG)\nsf36<-data.frame(CF,Limit,Dor,SG,VIT,AS,Emo,SM)\n\ncor(sst,sf36)\ncor<-as.table(cor(sst,sf36))\ncor.test(PA,CF,method=c(\"spearman\"))\ncor.test(PA,Limit,method=c(\"spearman\"))\ncor.test(PA,Dor,method=c(\"spearman\"))\ncor.test(PA,SG,method=c(\"spearman\"))\ncor.test(PA,VIT,method=c(\"spearman\"))\ncor.test(PA,AS,method=c(\"spearman\"))\ncor.test(PA,Emo,method=c(\"spearman\"))\ncor.test(PA,SM,method=c(\"spearman\"))\n\n#### IRT\n\ninstall.packages(\"ltm\")\nlibrary(ltm)\n\nLSAT\ndescript(LSAT) #descriptives\n\n\nfit1 <- rasch(LSAT, constraint = cbind(length(LSAT) + 1, 1)) #fitting rasch model\nsummary(fit1) #Show the summary for the rasch model fitting\n\ncoef(fit1, prob = TRUE, order = TRUE) #Transform de difficulties into probabilites\n\nGoF.rasch(fit1, B = 199) #Check the fit of the model\n\nmargins(fit1) #Alternative fit check, using two or threee way X2 residuals\nmargins(fit1, type = \"three-way\", nprint = 2) #fitting for 3way ckech \n\nfit2 <- rasch(LSAT) #Unconstrained method/ 1 slope for the whole data\nsummary(fit2)\n\nanova(fit1,fit2)#comparing discrimination rates\n\nmargins(fit2, type = \"three-way\", nprint = 2)\n\nfit3 <- ltm(LSAT ~ z1) #a Two=parameter logistic model - LSAT=dichotomus variavle/z1=latent variable\nsummary (fit3)\nanova(fit2, fit3)\n\nfit4 <- tpm(LSAT, type = \"rasch\", max.guessing = 1) #Three Parameter logistic model with a guessing parameter\nsummary(fit4)\nanova(fit2, fit4)\n\npar(mfrow = c(2, 2))\nplot(fit2, legend = TRUE, cx = \"bottomright\", lwd = 3,\n     cex.main = 1.5, cex.lab = 1.3, cex = 1.1)\nplot(fit2, type = \"IIC\", annot = FALSE, lwd = 3, cex.main = 1.5,\n     cex.lab = 1.3)\nplot(fit2, type = \"IIC\", items = 0, lwd = 3, cex.main = 1.5,\n     cex.lab = 1.3)\nplot(0:1, 0:1, type = \"n\", ann = FALSE, axes = FALSE)\ninfo1 <- information(fit2, c(-4, 0))\ninfo2 <- information(fit2, c(0, 4))\ntext(0.5, 0.5, labels = paste(\"Total Information:\", round(info1$InfoTotal, 3),\n                              \"\\n\\nInformation in (-4, 0):\", round(info1$InfoRange, 3),\n                              paste(\"(\", round(100 * info1$PropRange, 2), \"%)\", sep = \"\"),\n                              \"\\n\\nInformation in (0, 4):\", round(info2$InfoRange, 3),\n                              paste(\"(\", round(100 * info2$PropRange, 2), \"%)\", sep = \"\")), cex = 1.5)\n\nfactor.scores(fit2) #ability to estimate \nfactor.scores(fit2, resp.patterns = rbind(c(0,1,1,0,0), c(0,1,0,1,0))) #ability for nonspecific responde patterns\n\n#IRT for ordinal variables\nrcor.test(Environment, method = \"kendall\") #non parametric correlation, just ans alternativo to de descriptive function\n\nfit1 <- grm(Environment, constrained = TRUE)\nfit1\n\nmargins(fit1)\nmargins(fit1, type = \"three\")\n\nfit2 <- grm(Environment)\nfit2\n\nanova(fit1, fit2)\n\ninformation(fit2, c(-4, 4)) #check numerically the probabilities\n\ninformation(fit2, c(-4, 4), items = c(1, 6))\n\npar(mfrow = c(2, 2))\nplot(fit2, lwd = 2, cex = 1.2, legend = TRUE, cx = \"left\",\n     + xlab = \"Latent Trait\", cex.main = 1.5, cex.lab = 1.3, cex.axis = 1.1)\nplot(fit2, type = \"IIC\", lwd = 2, cex = 1.2, legend = TRUE, cx = \"topleft\",\n     + xlab = \"Latent Trait\", cex.main = 1.5, cex.lab = 1.3, cex.axis = 1.1)\nplot(fit2, type = \"IIC\", items = 0, lwd = 2, xlab = \"Latent Trait\",\n     + cex.main = 1.5, cex.lab = 1.3, cex.axis = 1.1)\ninfo1 <- information(fit2, c(-4, 0))\ninfo2 <- information(fit2, c(0, 4))\ntext(-1.9, 8, labels = paste(\"Information in (-4, 0):\",\n                             + paste(round(100 * info1$PropRange, 1), \"%\", sep = \"\"),\n                             + \"\\n\\nInformation in (0, 4):\",\n                             + paste(round(100 * info2$PropRange, 1), \"%\", sep = \"\")), cex = 1.2)\n\nfit1 <- ltm(WIRS ~ z1 + z2)\nfit2 <- ltm(WIRS ~ z1 * z2) #interaction between the latent variables improved the fit\nfit2\nanova(fit1, fit2)\n\npar(mfrow = c(2, 2))\nplot(fit2, category = 1, lwd = 2, cex = 1.2, legend = TRUE, cx = -4.5,\n     + cy = 0.85, xlab = \"Latent Trait\", cex.main = 1.5, cex.lab = 1.3,\n     + cex.axis = 1.1)\nfor (ctg in 2:3) {\n  + plot(fit2, category = ctg, lwd = 2, cex = 1.2, annot = FALSE,\n         + xlab = \"Latent Trait\", cex.main = 1.5, cex.lab = 1.3,\n         + cex.axis = 1.1)\n  + }\n",
    "created" : 1339418751240.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "hash" : "1706273730",
    "id" : "950253D3",
    "lastKnownWriteTime" : 1339419052,
    "path" : "C:/Users/user/Google Drive/Projeto_Duke/backpain_mturk/Validation_Script.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}